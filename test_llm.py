import os
from transformers import AutoProcessor, Gemma3ForConditionalGeneration, StoppingCriteria, StoppingCriteriaList
from pathlib import Path
import torch
import threading
import sounddevice as sd
import soundfile as sf
from kokoro import KPipeline  # ä½ çš„ TTS æ¨¡å‹
import numpy as np
import soundfile as sf
import tempfile
import time
import queue
import torchaudio



BASE_DIR = Path(__file__).resolve().parent
MODEL_DIR = BASE_DIR / "src" / "models" / "llm_models" / "gemma-3-4b-it"

model_name = "google/gemma-3-4b-it"
model_path = str(MODEL_DIR)  # è‡ªè¨‚æœ¬åœ°å„²å­˜ä½ç½®
torch.cuda.empty_cache()  # æ¸…é™¤ CUDA ç·©å­˜

# # **ç¬¬ä¸€æ¬¡åŸ·è¡Œæ™‚ä¸‹è¼‰ä¸¦å­˜æ”¾**
# tokenizer = AutoTokenizer.from_pretrained(model_path,  local_files_only=True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_path, 
#     local_files_only=True,
#     device_map="auto",  # è‡ªå‹•é¸æ“‡ GPU æˆ– CPU
#     torch_dtype=torch.float16  # ä½¿ç”¨ FP16 ä¾†ç¯€çœ VRAM
# )

# æ¸¬è©¦å°è©±è¨Šæ¯
messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a helpful assistant."}]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "I'm trying to practice my english speaking skills. Could you help me?"}
        ]
    }
]

# ğŸ”¥ åŠ è¼‰ LLMï¼ˆGemma 3ï¼‰
processor = AutoProcessor.from_pretrained(model_path, local_files_only=True)
model = Gemma3ForConditionalGeneration.from_pretrained(
    model_path, local_files_only=True, torch_dtype=torch.bfloat16
).to("cuda").eval()

# ğŸ”¥ åŠ è¼‰ TTSï¼ˆKokoroï¼‰
TTS_MODEL_DIR = BASE_DIR / "src" / "models" / "tts_models"
pipeline = KPipeline(lang_code='a')
voice_path = os.path.join(TTS_MODEL_DIR, "voices/af_heart.pt")
voice_tensor = torch.load(voice_path, weights_only=True)

# è™•ç†è¼¸å…¥
inputs = processor.apply_chat_template(
    messages, add_generation_prompt=True, tokenize=True,
    return_dict=True, return_tensors="pt"
).to(model.device, dtype=torch.bfloat16 )

# è¨ˆç®—è¼¸å…¥é•·åº¦ï¼ˆç¢ºä¿è¼¸å‡ºæ™‚ä¸åŒ…å«è¼¸å…¥å…§å®¹ï¼‰
input_len = inputs["input_ids"].shape[-1]

start_time = time.time()

# æ¨ç†
# with torch.inference_mode():
#     generation = model.generate(
#         **inputs,
#         max_new_tokens=100,  # âœ… é™ä½è¼¸å‡ºé•·åº¦ï¼ŒåŠ å¿«æ¨ç†
#         temperature=0.7,  # âœ… å¢åŠ éš¨æ©Ÿæ€§
#         top_k=50,  # âœ… é™ä½è¨ˆç®—é–‹éŠ·
#         top_p=0.9,  # âœ… æ¡æ¨£æ©Ÿåˆ¶
#         do_sample=True,  # âœ… å•Ÿç”¨éš¨æ©ŸæŠ½æ¨£
#         use_cache=True,  # âœ… å•Ÿç”¨ cacheï¼ŒåŠ é€Ÿ token ç”Ÿæˆ
#         stream=True
#     )
#     generation = generation[0][input_len:]  # åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†


# ğŸš€ **ä½¿ç”¨ StoppingCriteria ä¾†å¯¦ç¾é€æ­¥è¼¸å‡º**
# class StreamStoppingCriteria(StoppingCriteria):
#     def __init__(self, max_new_tokens):
#         self.max_new_tokens = max_new_tokens
#         self.token_count = 0

#     def __call__(self, input_ids, scores, **kwargs):
#         new_token = input_ids[:, -1]
#         print(processor.decode(new_token, skip_special_tokens=True), end="", flush=True)
#         self.token_count += 1
#         return self.token_count >= self.max_new_tokens

# # ğŸš€ **è®“ `generate()` é€æ­¥è¼¸å‡º**
# with torch.inference_mode():
#     model.generate(
#         **inputs,
#         max_new_tokens=100,  # âœ… è¨­å®šæœ€å¤§è¼¸å‡ºé•·åº¦
#         temperature=0.7,  
#         top_k=50,  
#         top_p=0.9,  
#         do_sample=True,  
#         use_cache=True,  
#         stopping_criteria=StoppingCriteriaList([StreamStoppingCriteria(100)])  # ğŸš€ é€æ­¥è¼¸å‡º Token
#     )

# ğŸš€ **TTS é€å€‹å­—ç™¼éŸ³**
# def play_tts(text):
#     global voice_tensor
#     if text.strip() == "":
#         return

#     try:
#         generator = pipeline(text, voice=voice_tensor, speed=1.0)
#         all_audio = []
#         for _, _, audio in generator:
#             all_audio.append(audio)

#         if all_audio:
#             full_audio = np.concatenate(all_audio)

#             # âœ… ç›´æ¥æ’­æ”¾éŸ³é »ï¼ˆä¸å­˜æª”æ¡ˆï¼‰
#             sd.play(full_audio, samplerate=24000)
#             sd.wait()  # âœ… ç¢ºä¿æ’­æ”¾å®Œç•¢

#     except Exception as e:
#         print(f"\nTTS éŒ¯èª¤: {e}")
# ğŸš€ **TTS è®€å–ä¸¦æ’­æ”¾éŸ³é »ï¼ˆç¢ºä¿æ’éšŠæ’­æ”¾ï¼‰**

# def remove_silence(audio, sr=24000):
#     """
#     ä½¿ç”¨ torchaudio çš„ VAD (Voice Activity Detection) ä¾†å»é™¤éŸ³é »éœéŸ³éƒ¨åˆ†
#     """
#     # è½‰æ›æˆ torch tensorï¼Œç¢ºä¿æ ¼å¼æ­£ç¢º
#     audio_tensor = torch.tensor(audio, dtype=torch.float32)

#     # âœ… ä½¿ç”¨ torchaudio.transforms.Vad() å»é™¤éœéŸ³
#     vad = torchaudio.transforms.Vad(sample_rate=sr)

#     # **è™•ç†éŸ³é »ï¼Œæ¯æ¬¡åªä¿ç•™æœ‰èªéŸ³çš„éƒ¨åˆ†**
#     trimmed_audio = vad(audio_tensor)

#     # å¦‚æœå»é™¤å¾Œæ²’æœ‰è²éŸ³ï¼Œå‰‡è¿”å›åŸå§‹éŸ³é »ï¼Œé¿å…éåº¦åˆªé™¤
#     return trimmed_audio.numpy() if len(trimmed_audio) > 0 else audio

tts_queue = queue.Queue()
def tts_worker():
    while True:
        sentence = tts_queue.get()  # å–å‡ºæ’éšŠçš„å¥å­
        if sentence is None:
            break  # å¦‚æœæ˜¯ `None`ï¼Œä»£è¡¨çµæŸ

        try:
            print(f"=====TTS sentense< {sentence} >======")

            generator = pipeline(sentence, voice=voice_tensor, speed=1.25)
            all_audio = []
            for _, _, audio in generator:
                all_audio.append(audio)

            if all_audio:
                full_audio = np.concatenate(all_audio)

                # âœ… æ’­æ”¾éŸ³é »ï¼ˆç¢ºä¿ä¸è¢«è¦†è“‹ï¼‰
                sd.play(full_audio, samplerate=24000)
                sd.wait()  # âœ… **ç­‰å¾…æ’­æ”¾å®Œç•¢**

        except Exception as e:
            print(f"\nTTS éŒ¯èª¤: {e}")
            
import re

# ğŸš€ **è®“ LLM é€å­—è¼¸å‡ºï¼ŒåŒæ™‚å‚³çµ¦ TTS**
class StreamStoppingCriteria(StoppingCriteria):
    def __init__(self, max_new_tokens, min_sentence_length=8):
        self.max_new_tokens = max_new_tokens
        self.token_count = 0
        self.current_sentence = ""
        self.min_sentence_length = min_sentence_length  # âœ… **è‡³å°‘ 8 å€‹ token æ‰ç™¼é€ TTS**
        self.valid_chars_pattern = re.compile(r"[A-Za-z0-9.,!?]")  # âœ… **æ­£å‰‡è¡¨é”å¼ï¼Œéæ¿¾ç‰¹æ®Šç¬¦è™Ÿ**

    def __call__(self, input_ids, scores, **kwargs):
        new_token = input_ids[:, -1]
        decoded_text = processor.decode(new_token, skip_special_tokens=True)
        
        self.current_sentence += decoded_text
        print(decoded_text, end="", flush=True)

        # âœ… ç•¶é‡åˆ°æ¨™é»ç¬¦è™Ÿæ™‚ï¼Œé€çµ¦ TTS
        if decoded_text in [".", "!", "?", ","] and len(self.current_sentence) >= self.min_sentence_length:
            sentence = "".join(self.current_sentence).strip()
            self.current_sentence = []  # æ¸…ç©ºç›®å‰ç´¯ç©çš„å¥å­
            tts_queue.put(sentence)  # âœ… **å°‡å¥å­åŠ å…¥ TTS ä½‡åˆ—**
            
        self.token_count += 1
        return self.token_count >= self.max_new_tokens


# è§£ç¢¼ä¸¦è¼¸å‡ºçµæœ
# decoded = processor.decode(generation, skip_special_tokens=True)
# âœ… **å•Ÿå‹• TTS æ’­æ”¾åŸ·è¡Œç·’**
tts_thread = threading.Thread(target=tts_worker, daemon=True)
tts_thread.start()

# ğŸš€ **è®“ LLM é€æ­¥è¼¸å‡ºä¸¦åŒæ­¥ TTS**
print("ç”Ÿæˆçµæœï¼š", end="", flush=True)
with torch.inference_mode():
    model.generate(
        **inputs,
        max_new_tokens=200,  
        temperature=0.7,  
        top_k=50,  
        top_p=0.9,  
        do_sample=True,  
        use_cache=True,  
        stopping_criteria=StoppingCriteriaList([StreamStoppingCriteria(200)])  # ğŸš€ é€æ­¥è¼¸å‡º Token
    )


end_time = time.time()
elapsed_time = end_time - start_time
print(f"TEXTç”ŸæˆèŠ±è²»æ™‚é–“: {elapsed_time:.2f} ç§’")

# âœ… **ç­‰å¾…æ‰€æœ‰ TTS æ’­æ”¾å®Œæˆ**
tts_queue.put(None)  # è®“ TTS åŸ·è¡Œç·’çµæŸ
tts_thread.join()

# print(decoded)